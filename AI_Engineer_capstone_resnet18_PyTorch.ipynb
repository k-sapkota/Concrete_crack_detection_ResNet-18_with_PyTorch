{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1><h1>Crack detection on concrete images using pretrained ResNet-18, a deep convolution neural network (CNN) model, with pytorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Author: Keshab Sapkota"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Abstract"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "I completed this labwork as my **AI capstone project with Deep Learning** for IBM AI Engineering Professional Certificate. In this work, I used pretrained ResNet-18 model to detect cracks on concrete images. The dataset contained 40000 RGB images of cracked and uncracked concrete surfaces which were saved in pytorch tensor format. The cracked concrete images or positive samples were labeled with '1', and uncracked images or negative samples were labeled with '0'. The Dataset class was constructed to set 30000 images for training and 10000 images for validation. The challenge associated with this dataset was that cracks could be confused with noise in the background texture, foreign objects, illumination, and irregularities such as exposure of jointing and finding. To solve this problem of cracks detection, I used Pre-trained ResNet-18 model which is a state of art convolutional neural network (CNN) model that is 18 layers deep and has beed trained with over 1 million images. ResNet-18 model for this project was prepared by adding a custom fully connected final hidden neural network layer and an feature extractor output layer on the pretrained convolution neural network layers. The fully connected layer was a linear classifier which contained 512 inputs matching with the output of CNN output. The output layer contained two outputs corresponding to cracked and uncracked concrete lables. The model was trained on train dataset with batch processing by minimizing crossEntropyLoss as loss function. The model performance was examined by validation dataset and accuracy of the model was calculated which is over 98%. Finally, first few misclassified samples were also indentified.\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n<ol>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ol>\n\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. [Download Data](#Download-Data)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'torchvision'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-14-aa651bfafb96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# These are the libraries will be used for this lab.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
                    ]
                }
            ],
            "source": "# Follwing libraries are used here.\n\nimport torchvision.models as models\nfrom torchvision import transforms\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport time\nimport matplotlib.pylab as plt\nimport numpy as np\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Download Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Download the dataset and unzip the files in your data directory. This project was completed in IBM Watson studio. Some of the following codes are specific to Watson studio."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-04-03 01:22:47--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\n100%[====================================>] 2,598,656,062 40.7MB/s   in 57s    \n\n2020-04-03 01:23:45 (43.3 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip \n!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-04-03 01:26:06--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\n100%[====================================>] 2,111,408,108 44.3MB/s   in 46s    \n\n2020-04-03 01:26:52 (44.0 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's install torchvision."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting torchvision\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0MB 5.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.12.0)\nCollecting torch==1.4.0 (from torchvision)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 753.4MB 43kB/s s eta 0:00:01              | 13.9MB 29.0MB/s eta 0:00:26     |\u2588                               | 21.9MB 29.0MB/s eta 0:00:26     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e             | 430.9MB 22.6MB/s eta 0:00:15\n\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.4.0 torchvision-0.5.0\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Explore Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Following code sets paths for positive (cracked concrete images) and negative files (uncracked concrete images)"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "#Paths of positive and negative files\ndirectory=\"/home/dsxuser/work\"\npositive=\"Positive_tensors\"\nnegative='Negative_tensors'\n\npositive_file_path=os.path.join(directory,positive)\nnegative_file_path=os.path.join(directory,negative)\npositive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\nnegative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'torch' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-8-5902ab8503a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
                    ]
                }
            ],
            "source": "print(positive_files[0])\nprint(negative_files[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "torch.load(positive_files[0])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Transform image data for the plot**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Define transform function to get image data that can be plotted in plt.imshow\n\ndef transform(torch_image, isfile = False):\n    \n    #torch_image has shape of (3, xxx, xxx)\n    if isfile == True:\n        np_image = torch.load(torch_image).numpy()\n    else:\n        np_image = torch_image.numpy()\n        \n    d1, d2 = np_image.shape[1], np_image.shape[2]\n    \n    image = np.zeros([d1, d2, 3])\n    for i, layer in enumerate(np_image):\n        #transform data in the range [0,1]\n        transformed = (layer - np.min(layer))/(np.max(layer)- np.min(layer))\n        \n        #Transform into shape (xxx, xxx, 3) which can be plotted by plt.imshow\n        image[:, :, i] = transformed\n        \n    return(image)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Plot images**"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[[1.0501755 , 1.0330508 , 1.015926  , ..., 1.1871736 ,\n         1.1871736 , 1.1871736 ],\n        [1.0673003 , 1.0330508 , 0.9816765 , ..., 1.1700488 ,\n         1.1700488 , 1.1700488 ],\n        [1.0673003 , 1.015926  , 0.96455175, ..., 1.1700488 ,\n         1.1700488 , 1.1700488 ],\n        ...,\n        [1.1015497 , 1.1015497 , 1.1186745 , ..., 1.1700488 ,\n         1.2042983 , 1.2385478 ],\n        [1.0673003 , 1.0673003 , 1.0844251 , ..., 1.1700488 ,\n         1.2042983 , 1.2385478 ],\n        [1.0501755 , 1.0330508 , 1.0501755 , ..., 1.1700488 ,\n         1.2042983 , 1.2385478 ]],\n\n       [[1.1155462 , 1.0980393 , 1.0805323 , ..., 1.2906163 ,\n         1.2906163 , 1.2906163 ],\n        [1.1330533 , 1.0980393 , 1.0455183 , ..., 1.2731093 ,\n         1.2731093 , 1.2731093 ],\n        [1.1330533 , 1.0805323 , 1.0280112 , ..., 1.2731093 ,\n         1.2731093 , 1.2731093 ],\n        ...,\n        [1.1680672 , 1.1680672 , 1.1855743 , ..., 1.2731093 ,\n         1.3081232 , 1.3431373 ],\n        [1.1330533 , 1.1330533 , 1.1505603 , ..., 1.2731093 ,\n         1.3081232 , 1.3431373 ],\n        [1.1155462 , 1.0980393 , 1.1155462 , ..., 1.2731093 ,\n         1.3081232 , 1.3431373 ]],\n\n       [[1.2282355 , 1.2108063 , 1.193377  , ..., 1.3850982 ,\n         1.3850982 , 1.3850982 ],\n        [1.2456646 , 1.2108063 , 1.1585187 , ..., 1.367669  ,\n         1.367669  , 1.367669  ],\n        [1.2456646 , 1.193377  , 1.1410894 , ..., 1.367669  ,\n         1.367669  , 1.367669  ],\n        ...,\n        [1.3153814 , 1.3153814 , 1.3328106 , ..., 1.367669  ,\n         1.4025275 , 1.4373858 ],\n        [1.2805231 , 1.2805231 , 1.2979523 , ..., 1.367669  ,\n         1.4025275 , 1.4373858 ],\n        [1.2630938 , 1.2456646 , 1.2630938 , ..., 1.367669  ,\n         1.4025275 , 1.4373858 ]]], dtype=float32)"
                    },
                    "execution_count": 71,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#Display positive files\nf, ax = plt.subplots(1, 3, figsize = (10, 10))\nprint('Positive files (crack on conceret)')\nfor i in range(3):\n    image = transform(positive_files[i], isfile = True)\n    ax[i].imshow(image)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Display negative files\nf, ax = plt.subplots(1, 3, figsize = (10, 10))\nprint('Negative files (no crack on concrete)')\nfor i in range(3):\n    image = transform(negative_files[i], isfile = True)\n    ax[i].imshow(image)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">3. Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " Here Dataset class is constructed which defines training and validation datasets. The each dataset will have positive and negative samples alternately."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/home/dsxuser/work\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Next, let's create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Training and Validation dataset objects\n\ntrain_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Check a tensor from train_dataset, it should have image data and y label\ntrain_dataset[0]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Check if the image data is of required shape and type\nprint(train_dataset[0][0].shape)\nprint(train_dataset[0][0].type())\ntorch.mean(torch.mean(train_dataset[0][0], axis = 1), axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Display train images, it should have positive and negative smaples alternately\nf, ax = plt.subplots(1, 4, figsize = (12, 12))\n\nprint('Train images')\nfor i in range(4):\n    image = transform(train_dataset[i][0], isfile = False)\n    ax[i].imshow(image)\n    ax[i].set_title('Image lagel: y = ' + str(train_dataset[i][1].item()))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Display validation images, it should also have positive and negative smaples alternately\nf, ax = plt.subplots(1, 4, figsize = (12, 12))\n\nprint('Validation images')\nfor i in range(4):\n    image = transform(validation_dataset[i][0], isfile = False)\n    ax[i].imshow(image)\n    ax[i].set_title('Image lagel: y = ' + str(validation_dataset[i][1].item()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4. Prepare a pre-trained ResNet-18 model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Load the pre-trained model ResNet-18**"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e6c9d51a469940ffabf26193d1a47997",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "data": {
                        "text/plain": "'\\n#Normalizing the data\\nmean=[0.485, 0.456, 0.406]\\nstd=[0.229, 0.224, 0.225]\\n\\n#Since date is already in required shape i.e. torch.Size([3, 224, 224]) and type i.e. torch.FloatTensor, we skip that part here.\\ncomposed = transforms.Compose([transforms.Normalize(mean, std)])\\ntrain_dataset = Dataset(transform = composed, train = True)\\nvalidation_dataset = Dataset(transform = composed, train = False)'"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#Load ResNet-18 model\nmodel = models.resnet18(pretrained = True)\n\n#Normalizing parameters for 3 (RGB) layers, these values are obtained from ResNet-18 model\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\n#Normalize data\ncomposed = transforms.Compose([transforms.Normalize(mean, std)])\ntrain_dataset = Dataset(transform = composed, train = True)\nvalidation_dataset = Dataset(transform = composed, train = False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Since this model is pretrained, we don't want to train the weights in CNN layers. So, we set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the CNN parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "for param in model.parameters():\n    param.requires_grad = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Customize output layer**\n\nThe fully connected layer of ResNet-18 has 512 inputs from hidden CNN layers. ResNet-18 is used to classify 1000 different objects; as a result, the last layer has 1000 output i.e. it has 512 x 1000 fully connected neurons. Here, this fully connected layer is replaced by customized layer that has 512 inputs and 2 outputs."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "# Custom fully connected layer\nmodel.fc = nn.Linear(512, 2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's see how is our ResNet-18 model looks like"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 5. Train the Model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Create a cross entropy criterion function**"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "# The loss function\ncriterion = nn.CrossEntropyLoss()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Data loader with batch of 100 samples**: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "train_loader = DataLoader(dataset = train_dataset, batch_size=100)\nvalidation_loader = DataLoader(dataset = validation_dataset, batch_size=100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Optimizer**"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Training for just one epoch (it takes more than 30 minuts!).  Accuracy is calculated on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.99"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n\nLoss=0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n\n        model.train() \n        #clear gradient\n        optimizer.zero_grad()         \n        #make a prediction \n        yhat = model(x)\n        # calculate loss \n        loss = criterion(yhat, y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        loss_list.append(loss.data)\n        \n    correct=0\n    for x_test, y_test in validation_loader:\n        # set model to eval \n        model.eval()\n        #make a prediction \n        z = model(x)\n        #find max indices\n        _, yhat = torch.max(z, axis = 1)\n       \n        #Calculate misclassified  samples in mini-batch (100 samples) \n        correct += (yhat==y_test).sum().item()\n   \n    accuracy=correct/N_test\n    \nend_time = time.time()    \nprint('Training started: 's, tart_time, '\\nTraining copleted:', end_time)\nprint('Accuracy of the model: ', accuracy)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Loss plot**"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "#Loss vs batch iteration of training data\nplt.plot(loss_list)\nplt.xlabel(\"Iteration\", fontweight = 'bold')\nplt.ylabel(\"Loss\", fontweight = 'bold)\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_3\">6. Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Sample 390 predicted value: 0, actual value 1\nSample 768 predicted value: 0, actual value 1\nSample 794 predicted value: 0, actual value 1\nSample 863 predicted value: 1, actual value 0\nSample 974 predicted value: 0, actual value 1\n"
                }
            ],
            "source": "f, ax = plt.subplots(1, 4, figsize = (12, 12))\n\nn = 0\nfor i, (x_val, y_val) in enumerate(validation_loader):\n        #make a prediction \n        z = model(x_val)\n        #find max \n        _, yhat = torch.max(z, axis = 1)\n        \n        missed_indices = np.where(yhat != y_val)[0]\n        \n        #Plot missed images\n        for ind in missed_indices:\n            n += 1\n            title = 'Sample {} \\npredicted value {}, \\nactual value {}'.\\\n                  format(ind + 100*i, yhat[ind], y_val[ind])\n            image = transform(x_val[ind], isfile = False)\n            ax[n-1].imshow(image)\n            ax[n-1].set_title(title)\n            if n >= 4:\n                break\n        if n >= 4:\n            break"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}